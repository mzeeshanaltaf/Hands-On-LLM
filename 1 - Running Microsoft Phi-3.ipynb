{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916792c7-ae17-4545-93b1-d8408476c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32a4941c-5381-4954-84fc-1c4f4dda510e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c8a086f8fb48598118bb82d61c7977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zeeshan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Zeeshan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-4K-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683eb861cb19475d826e28736b7a20c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4K-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377d07a7cc87451591c6933012e5bfd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4K-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a2f8d0141f4873a694106861a5b9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4e3cff8203412ebd64523b33f082b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2607759b84af4ad2a51b828aea2d58a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41bc9b4e0a341e6b3a8c1e0a2be86a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9c5a6effce49f5aa74192e0c95748c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4e8ba9f582400eaa39cf307708410e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0266a90ff44df3b4b21fad30449933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec50ed25130743dcbad14faca2b0c582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4b0e27c3c14492a1d9cfed7acfa2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef752373a0a4bb89ce93d8a8d93e1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8ba7d56fa343729e920ada4631939f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4K-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4K-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b85cf2-0ca1-4647-b736-b3b649847cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the tockenizer's vocab size\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a39c742-c948-47df-b7e1-7fe74c5e185f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Hello: [15043]\n",
      "Tokenized World: [2787]\n",
      "Tokenized AI: [319, 29902]\n",
      "Tokenized learning: [6509]\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer with some example words\n",
    "words = [\"Hello\", \"World\", \"AI\", \"learning\"]\n",
    "for word in words:\n",
    "    print(f\"Tokenized {word}: {tokenizer.encode(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "195adf3b-5c69-41f2-8e5c-2cd5bc348c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Artificial Intelligence is transforming the world.\n",
      "Tokenized Sentence: [3012, 928, 616, 3159, 28286, 338, 4327, 292, 278, 3186, 29889]\n",
      "Decoded Sentence: Artificial Intelligence is transforming the world.\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer with a sentence\n",
    "sentence = \"Artificial Intelligence is transforming the world.\"\n",
    "encoded_sentence = tokenizer.encode(sentence)\n",
    "decoded_sentence = tokenizer.decode(encoded_sentence)\n",
    "\n",
    "print(f'Original Sentence: {sentence}')\n",
    "print(f'Tokenized Sentence: {encoded_sentence}')\n",
    "print(f'Decoded Sentence: {decoded_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c4d2baf-25c7-407d-953f-cbb31c715aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: 3012, Token: Art\n",
      "Token ID: 928, Token: ific\n",
      "Token ID: 616, Token: ial\n",
      "Token ID: 3159, Token: Int\n",
      "Token ID: 28286, Token: elligence\n",
      "Token ID: 338, Token: is\n",
      "Token ID: 4327, Token: transform\n",
      "Token ID: 292, Token: ing\n",
      "Token ID: 278, Token: the\n",
      "Token ID: 3186, Token: world\n",
      "Token ID: 29889, Token: .\n"
     ]
    }
   ],
   "source": [
    "# Encode the sentence to get the token IDs\n",
    "encoded_sentence = tokenizer.encode(sentence)\n",
    "\n",
    "# Decode each token ID back to it's respective token\n",
    "tokens = [tokenizer.decode([token_id]) for token_id in encoded_sentence]\n",
    "\n",
    "# Create a mapping of token IDs to tokens\n",
    "token_id_map = list(zip(encoded_sentence, tokens))\n",
    "\n",
    "# Print the token IDs and their corresponding tokends\n",
    "for token_id, token in token_id_map:\n",
    "    print(f'Token ID: {token_id}, Token: {token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6d83186-8ee3-4d8f-9942-95c8f02407ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, return_full_text=False, max_new_tokens=500, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e02433c6-c6de-43a7-9caf-a4ca959b5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain Machine Learning to a Grandmother\"},\n",
    "]\n",
    "\n",
    "output = generator(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "797b142b-b7fb-4b74-94d0-5f22645dba4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Machine Learning is a special type of computer science that teaches computers to learn from data, just like how we learn from our experiences. Imagine you have a friend who is really good at recognizing faces. You show her a picture of your grandson, and she can tell you it's your grandson without you telling her his name. Machine Learning works in a similar way.\n",
      "\n",
      "Computers are given lots of pictures, names, and information about things. They learn to recognize patterns and make predictions based on what they've learned. For example, a computer can learn to recognize your grandson's face by looking at many pictures of him and noticing the unique features that make him different from other people.\n",
      "\n",
      "Machine Learning is used in many ways, like helping doctors diagnose diseases, recommending movies on streaming services, and even helping cars drive themselves. It's a powerful tool that helps us solve problems and make our lives easier.\n"
     ]
    }
   ],
   "source": [
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d03e74-7409-4379-8dc5-fd645c2fc985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
